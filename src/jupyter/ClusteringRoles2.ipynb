{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(folder_name, file_name, data):\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(folder_name)\n",
    "        print(\"Directory \" , folder_name ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        e = 1\n",
    "        #print(\"Directory \" , folder_name ,  \" already exists\")\n",
    "    data.to_csv(folder_name + \"/\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file_with_index(folder_name, file_name, data):\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(folder_name)\n",
    "        print(\"Directory \" , folder_name ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        e = 1\n",
    "        #print(\"Directory \" , folder_name ,  \" already exists\")\n",
    "    data.to_csv(folder_name + \"/\" + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [(\"Old_data/FrequencyOfUserPostsWithoutZeros\",\"std_post_frequency\"),\n",
    "            (\"Old_data/FrequencyOfUserPostsWithoutZeros\",\"q3_post_frequency\"),\n",
    "            (\"Old_data/NumberOfReceivedResponsesToUsersPostsWithoutZeros\",\"number_of_received_responses_to_users_posts_std\"),\n",
    "            (\"Old_data/NumberOfReceivedResponsesToUsersPostsWithoutZeros\",\"number_of_received_responses_to_users_posts_max\"),\n",
    "            (\"Old_data/NumberOfReceivedResponsesUnderUsersCommentsWithoutZeros\", \"number_of_received_responses_under_users_comments_q3\"),\n",
    "            (\"Old_data/NumberOfReceivedResponsesUnderUsersCommentsWithoutZeros\",\"number_of_received_responses_under_users_comments_max\"),\n",
    "            (\"Old_data/NumberOfWordsInOwnResponsesOfUsersPostsWithoutZeros\", \"number_of_words_in_own_responses_of_users_posts_q3\"),\n",
    "            (\"Old_data/NumberOfWordsInResponsesOfUsersPostsWithoutZeros\",\"nnumber_of_words_in_responses_of_users_posts_median\"),\n",
    "            (\"Old_data/SentimentOfUsersPostsWithoutZeros\",\"posts_sentiment_min\"),\n",
    "            (\"Old_data/FrequencyOfUserCommentsWithoutZeros\", \"mean_comments_frequency\"),\n",
    "            (\"Old_data/NumberOfWordsInUsersCommentsWithoutZeros\",\"number_of_words_in_users_comments_avg\"),\n",
    "            (\"Old_data/NumberOfWordsInUsersPostsWithoutZeros\", \"number_of_words_in_users_posts_q3\"),\n",
    "            (\"Old_data/NumberOfReceivedResponsesToUsersPostsWithoutZeros\", \"number_of_received_responses_to_users_posts_q3\"),\n",
    "            (\"Old_data/NumberOfCommentsWrittenByUserUnderHisOwnPostsWithoutZeros\", \"number_of_comments_written_by_user_under_his_own_posts_q3\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_comments = datetime.date(2008, 12, 9)\n",
    "end_date_comments = datetime.date(2013,11, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"Old_data/NumberOfReceivedResponsesToUsersPostsWithoutZeros\" + \"/feature_\" + str(start_date_comments) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stats(start_date):\n",
    "    statistics_to_join = []\n",
    "    for feat in features:\n",
    "        (file_name, feature) = feat\n",
    "        statistics_to_join.append(pd.read_csv(file_name + \"/feature_\" + str(start_date) + \".csv\")[['user_id',feature]])\n",
    "    \n",
    "    merged_df = statistics_to_join[0]\n",
    "    for i in range(1, len(statistics_to_join)):\n",
    "        merged_df = pd.merge(merged_df, statistics_to_join[i],how='outer',on=['user_id'])\n",
    "    return merged_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_data(start_date, end_date):\n",
    "    res = merge_stats(start_date)\n",
    "    start_date += timedelta(days=14)\n",
    "    while start_date < end_date:\n",
    "        data = merge_stats(start_date)\n",
    "        res = pd.concat([res, data], ignore_index=True)\n",
    "        print(str(start_date))\n",
    "        #print(str(len(data)) + \" \" + str(start_date))\n",
    "        \n",
    "        start_date += timedelta(days=14)\n",
    "    save_data_to_file(\"Cluster_Data_All\", \"cluster.csv\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_cluster_data(start_date_comments, end_date_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine k = 7\n",
    "\n",
    "# columns = ['start_date',\n",
    "#            'user_id',\n",
    "#            'posts_activity_time',\n",
    "#            'frequency_of_posts_avg',\n",
    "#            'frequency_of_posts_stddev',\n",
    "#            'frequency_of_comments_stddev',\n",
    "#            'frequency_of_comments_q3',\n",
    "#            'number_of_received_responses_to_post_max',\n",
    "#            'number_of_received_responses_to_post_stddev',\n",
    "#            'number_of_received_responses_to_post_avg',\n",
    "#            'number_of_received_responses_under_comments_stddev',\n",
    "#            'number_of_received_responses_under_comments_q3',\n",
    "#            'number_of_received_responses_under_comments_max',\n",
    "#            'number_of_own_post_responses_max', \n",
    "#            'number_of_words_in_posts_q3',\n",
    "#            'number_of_words_in_comments_median',\n",
    "#            'number_of_words_in_responses_to_posts_q3',\n",
    "#            'number_of_words_in_own_post_responses_q3',\n",
    "#            'number_of_all_responses_from_unique_users_in_slot']\n",
    "\n",
    "# ,\n",
    "#            'post_sentiment_stddev',\n",
    "#            'post_sentiment_q3',\n",
    "#            'comments_sentiment_q3',\n",
    "#            'comments_sentiment_stddev',\n",
    "#            'received_response_sentiment_stddev'\n",
    "\n",
    "\n",
    "columns = ['start_date',\n",
    "           'user_id',\n",
    "           'posts_activity_time',\n",
    "           'frequency_of_posts_avg',\n",
    "           'frequency_of_posts_stddev',\n",
    "           'frequency_of_comments_q3',\n",
    "           'number_of_received_responses_to_post_max',\n",
    "           'number_of_received_responses_to_post_stddev',\n",
    "           'number_of_received_responses_to_post_avg',\n",
    "           'number_of_received_responses_to_post_q3',\n",
    "           'number_of_received_responses_under_comments_q3',\n",
    "           'number_of_received_responses_under_comments_max',\n",
    "           'number_of_own_post_responses_q3',\n",
    "           'number_of_words_in_posts_q3',\n",
    "           'number_of_words_in_comments_median',\n",
    "           'number_of_words_in_responses_to_posts_q3',\n",
    "           'number_of_words_in_own_post_responses_q3']\n",
    "\n",
    "# #            'post_sentiment_median',\n",
    "# #            'post_sentiment_min', 'comments_sentiment_median', 'received_response_sentiment_stddev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = pd.read_csv(\"FeatureTableCSV.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_comments = datetime.date(2009, 12, 22)\n",
    "end_date_comments = datetime.date(2013,10, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats_for_entire_data(feature_table, start_date, end_date):\n",
    "    results = pd.DataFrame()\n",
    "    while start_date < end_date:\n",
    "        features = pd.read_csv(\"All_Data_In_Slots_Joined/\" + str(start_date) + \"_joined.csv\")\n",
    "        features.insert(1, 'start_date', start_date)\n",
    "        results = pd.concat([results, features], ignore_index=True)\n",
    "        start_date += timedelta(days=14)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustering_data(columns, feature_table, start_date_comments, end_date_comments):\n",
    "    alldata = generate_stats_for_entire_data(feature_table, start_date_comments, end_date_comments)[columns]\n",
    "    save_data_to_file(\"Cluster_Data_All\", \"cluster.csv\", alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clustering_data(columns, feature_table, start_date_comments, end_date_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_clustering = datetime.date(2009, 12, 8)\n",
    "end_date_clustering = datetime.date(2013,10, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_custering(X):\n",
    "    columns=['k', 'score_mean', 'scored_std']\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    print(\"Benchmarking..\")\n",
    "    for k in range(4,11):\n",
    "        scores = []\n",
    "        for sample_num in range(0,10):\n",
    "            kmeans = KMeans(n_clusters=k).fit(X)\n",
    "            labels = kmeans.labels_\n",
    "            scores.append(metrics.calinski_harabasz_score(X, labels))\n",
    "            print(\"Clustering done for sample: \" + str(sample_num))\n",
    "        row = pd.DataFrame([[k, np.mean(scores), np.std(scores)]], columns=columns)\n",
    "        results = pd.concat([row, results], ignore_index=True)\n",
    "        print(\"Clustering done for: \" + str(k))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_benchmark_clustering_data():\n",
    "    df = pd.read_csv(\"Cluster_Data_All\" + \"/cluster.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    X = MinMaxScaler().fit_transform(df)\n",
    "    results = benchmark_custering(X)\n",
    "    save_data_to_file(\"Cluster_Benchmarks_All\", \"cluster_benchmark.csv\", results)\n",
    "    print(\"Results for {} saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_benchmark_clustering_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Cluster_Benchmarks_All/cluster_benchmark.csv\")\n",
    "\n",
    "x = df[\"k\"]\n",
    "y = df[\"score_mean\"]\n",
    "print(y)\n",
    "print(df[\"scored_std\"])\n",
    "yerr = df[\"scored_std\"]\n",
    "plt.errorbar(x,y,yerr=yerr)\n",
    "plt.show()\n",
    "# plt.errorbar('k', 'calinski_harabaz_score', yerr='std', data=df, marker='o', linestyle='solid',\n",
    "#              ecolor='darkred', capsize=3, markersize=3, capthick=2)\n",
    "# plt.xticks(range(2,16))\n",
    "# plt.xlabel('clusters number')\n",
    "# plt.ylabel('Calinski Harabaz score')\n",
    "# plt.title('K-means clustering evaluation')\n",
    "# plt.show()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data():\n",
    "    data_to_cluster = pd.read_csv(\"Cluster_Data_All\" + \"/cluster.csv\")\n",
    "    data_without_id = data_to_cluster.drop(columns =[\"user_id\", \"start_date\"])\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(data_without_id)\n",
    "    kmeans = KMeans(n_clusters=7).fit(data_without_id)\n",
    "    labels_df = pd.DataFrame(kmeans.labels_, columns=[\"label\"])\n",
    "    labeled_users = pd.concat([data_to_cluster, labels_df], axis=1)\n",
    "    cluster_df = pd.DataFrame(kmeans.cluster_centers_, columns=data_without_id.columns.values)\n",
    "    \n",
    "#     save_data_to_file(\"All_Labeled_users\", \"labeled_users.csv\", labeled_users)\n",
    "#     save_data_to_file(\"All_Cluster_centers\", \"cluster_centers.csv\", cluster_df)\n",
    "\n",
    "    save_data_to_file(\"All_Labeled_users\", \"with_new_feature_labeled_users.csv\", labeled_users)\n",
    "    save_data_to_file(\"All_Cluster_centers\", \"with_new_feature_cluster_centers.csv\", cluster_df)\n",
    "    \n",
    "    \n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     data_to_cluster = pd.read_csv(\"Cluster_Data\" + \"/cluster\" + row[\"date\"] + \".csv\")\n",
    "#     data_without_id = data_to_cluster.drop(columns =[\"user_id\"])\n",
    "#     data_to_cluster\n",
    "#     X = MinMaxScaler().fit_transform(data_without_id)\n",
    "#     kmeans = KMeans(n_clusters=7).fit(X)\n",
    "#     labels_df = pd.DataFrame(kmeans.labels_, columns=[\"label\"])\n",
    "#     labeled_users = pd.concat([data_to_cluster, labels_df], axis=1)\n",
    "#     cluster_df = pd.DataFrame(kmeans.cluster_centers_, columns=data_without_id.columns.values)\n",
    "    \n",
    "#     save_data_to_file(\"Labeled_users\", \"labeled_users\"+ row[\"date\"] +\".csv\", labeled_users)\n",
    "#     save_data_to_file(\"Cluster_centers\", \"cluster_centers\"+ row[\"date\"] +\".csv\", cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"All_Labeled_users\" + \"/labeled_users.csv\")[[\"user_id\",\"label\",\"start_date\"]]\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normalized_statistics():\n",
    "    stats = [np.min, np.mean, np.max, np.std]\n",
    "    column_names = columns={'mean': 'mean','min': 'amin', 'max': 'amax', 'std':'std'}\n",
    "    \n",
    "    #get sample feature names and create aggregats eg {'std_post_frequency': [np.mean, np.std, np.min, np.max]}\n",
    "    df = pd.read_csv(\"All_Labeled_users\" + \"/labeled_users.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    features = df.drop(columns =[\"label\"]).columns.values\n",
    "    aggreagates = { feat : stats for feat in features }\n",
    "    \n",
    "#     df = pd.read_csv(\"Labeled_users\" + \"/labeled_users\" + str(start_date) + \".csv\").drop(columns =[\"user_id\"])\n",
    "        \n",
    "    stats = (df.groupby(['label']).agg(aggreagates).rename(column_names))\n",
    "    all_feature = pd.read_csv(\"Cluster_Data_All\" + \"/cluster.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    divisors = []\n",
    "    for feat in all_feature.columns.values:\n",
    "        max_v = all_feature[feat].max()\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "    normalized = stats.div(divisors, axis='columns')\n",
    "    return normalized.T\n",
    "#     stats_trans_df = stats.T\n",
    "        \n",
    "#     save_data_to_file_with_index(\"All_Normalized_ClustersStatistics\", \"clusters_stats.csv\", stats_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = generate_normalized_statistics().round(5)\n",
    "display(HTML(norm_stats.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_stats.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normalized_statistics_with_new_feature():\n",
    "    stats = [np.min, np.mean, np.max, np.std]\n",
    "    column_names = columns={'mean': 'mean','min': 'amin', 'max': 'amax', 'std':'std'}\n",
    "    \n",
    "    #get sample feature names and create aggregats eg {'std_post_frequency': [np.mean, np.std, np.min, np.max]}\n",
    "    df = pd.read_csv(\"All_Labeled_users\" + \"/with_new_feature_labeled_users.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    features = df.drop(columns =[\"label\"]).columns.values\n",
    "    aggreagates = { feat : stats for feat in features }\n",
    "    \n",
    "#     df = pd.read_csv(\"Labeled_users\" + \"/labeled_users\" + str(start_date) + \".csv\").drop(columns =[\"user_id\"])\n",
    "        \n",
    "    stats = (df.groupby(['label']).agg(aggreagates).rename(column_names))\n",
    "    all_feature = pd.read_csv(\"Cluster_Data_All\" + \"/cluster.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    divisors = []\n",
    "    for feat in all_feature.columns.values:\n",
    "        max_v = all_feature[feat].max()\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "        divisors.append(max_v)\n",
    "    normalized = stats.div(divisors, axis='columns')\n",
    "    return normalized.T\n",
    "#     stats_trans_df = stats.T\n",
    "        \n",
    "#     save_data_to_file_with_index(\"All_Normalized_ClustersStatistics\", \"clusters_stats.csv\", stats_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = generate_normalized_statistics_with_new_feature().round(5)\n",
    "display(HTML(norm_stats.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(norm_stats.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custer_statistics():\n",
    "    stats = [np.mean, np.std, np.min, np.max]\n",
    "    column_names = columns={'mean': 'mean','std': 'stddev', 'amin': 'min', 'amax': 'max'}\n",
    "    \n",
    "    #get sample feature names and create aggregats eg {'std_post_frequency': [np.mean, np.std, np.min, np.max]}\n",
    "    df = pd.read_csv(\"All_Labeled_users\" + \"/labeled_users.csv\").drop(columns =[\"user_id\", \"start_date\"])\n",
    "    features = df.drop(columns =[\"label\"]).columns.values\n",
    "    aggreagates = { feat : stats for feat in features }\n",
    "    \n",
    "#     df = pd.read_csv(\"Labeled_users\" + \"/labeled_users\" + str(start_date) + \".csv\").drop(columns =[\"user_id\"])\n",
    "        \n",
    "    stats = (df.groupby(['label']).agg(aggreagates).rename(column_names))\n",
    "    stats_trans_df = stats.T\n",
    "        \n",
    "    save_data_to_file_with_index(\"All_ClustersStatistics\", \"clusters_stats.csv\", stats_trans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_custer_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"All_ClustersStatistics\" + \"/clusters_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"All_ClustersStatistics\" + \"/clusters_stats.csv\")\n",
    "display(HTML(df.reset_index(level=[0]).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.reset_index(level=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = [str(i) for i in range(0, 7)]\n",
    "clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters = [str(i) for i in range(0, 7)]\n",
    "base = [\"index\", \"feature\", \"stat\"]\n",
    "all_col = base.extend(all_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_cluster_data(df2):\n",
    "    df2.columns = [\"index\", \"feature\", \"stat\", \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "    all_feature = df2['feature'].drop_duplicates()\n",
    "    all_clusters = df2.columns.values\n",
    "    print(all_clusters)\n",
    "    for clust in all_clusters:\n",
    "        data = []\n",
    "        for feat in all_feature:\n",
    "            print(clust)\n",
    "            stats = df2.loc[df2[\"feature\"] == feat][clust]\n",
    "            row = [feat]\n",
    "            row.extend(stats)\n",
    "            data.append(row)\n",
    "\n",
    "        stat_df = pd.DataFrame(data, columns = [\"feature_name\", \"mean\", \"std\", \"min\", \"max\"])\n",
    "        save_data_to_file_with_index(\"ClusterSeparetStats\", str(clust) + \"_clusters_stats.csv\", stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperate_cluster_data(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = pd.read_csv(\"ClusterSeparetStats\" + \"/0_clusters_stats.csv\")\n",
    "display(HTML(cluster_0.to_html()))\n",
    "#print(cluster_0.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = pd.read_csv(\"ClusterSeparetStats\" + \"/1_clusters_stats.csv\")\n",
    "display(HTML(cluster_1.to_html()))\n",
    "#print(cluster_1.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2 = pd.read_csv(\"ClusterSeparetStats\" + \"/2_clusters_stats.csv\")\n",
    "display(HTML(cluster_2.to_html()))\n",
    "#print(cluster_2.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3 = pd.read_csv(\"ClusterSeparetStats\" + \"/3_clusters_stats.csv\")\n",
    "display(HTML(cluster_3.to_html()))\n",
    "#print(cluster_3.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_4 = pd.read_csv(\"ClusterSeparetStats\" + \"/4_clusters_stats.csv\")\n",
    "display(HTML(cluster_4.to_html()))\n",
    "#print(cluster_4.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5 = pd.read_csv(\"ClusterSeparetStats\" + \"/5_clusters_stats.csv\")\n",
    "display(HTML(cluster_5.to_html()))\n",
    "#print(cluster_5.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_6 = pd.read_csv(\"ClusterSeparetStats\" + \"/6_clusters_stats.csv\")\n",
    "display(HTML(cluster_6.to_html()))\n",
    "#print(cluster_6.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_7 = pd.read_csv(\"ClusterSeparetStats\" + \"/7_clusters_stats.csv\")\n",
    "print(cluster_7.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_8 = pd.read_csv(\"ClusterSeparetStats\" + \"/8_clusters_stats.csv\")\n",
    "print(cluster_8.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
